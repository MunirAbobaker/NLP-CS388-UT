\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amsfonts} 
\usepackage{comment}
%\usepackage[dvipsnames]{xcolor}
\DeclareMathOperator*{\argmax}{argmax}
\usepackage{url}
%\aclfinalcopy % Uncomment this line for the final submission
%%\def\aclpaperid{***} %  Enter the acl Paper ID here
\setlength\titlebox{3cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Discrete White Box Adversarial Attacks in a\\ Continuous Optimization Framework}

\author{Anish Acharya \\
 University of Texas at Austin\\
 %Affiliation / Address line 2 \\
  %Affiliation / Address line 3 \\
  \texttt{anishacharya@utexas.edu} \\\And
 Rudrajit Das \\
 University of Texas at Austin \\
  %Affiliation / Address line 2 \\
  %Affiliation / Address line 3 \\
  \texttt{rdas@cs.utexas.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
We consider the problem of efficiently generating high quality adversarial examples in NLP. 
%We will attempt to tackle two problems. 
%The first one is formulating 
%Specifically, we formulate the problem of finding adversarial examples by replacing words with their synonyms or sentences with equivalent paraphrases, as a continuous optimization problem rather than a discrete optimization problem. 
The problem of finding adversarial examples by replacing words with their synonyms or sentences with equivalent paraphrases is a discrete (combinatorial) optimization problem. We relax this into an approximately equivalent continuous optimization problem, which can be efficiently solved using gradient-based methods.
%The second one is going beyond replacing words with their synonyms (for finding adversarial examples) by using sentence level paraphrases generated by a pre-trained paraphrasing system. Further, the first framework can be used in conjunction with the second one for finding adversarial examples at document-level.
\end{abstract}

\section{Introduction}
Adversarial examples are carefully crafted modifications to the input, %while being imperceptible to humans could completely change the output of a classifier. 
which are imperceptible to humans but could completely change the output of an AI system. 
Adversarial examples pose a serious %AI 
safety concern as they could be used to attack any machine learning model with high success rate. %even without any access to the underlying model.
Therefore, training AI systems with self-generated adversarial examples makes them more robust. 

\section{Background}
In recent years, there has been a lot of effort to develop adversarial defense strategies to make ML models robust to adversarial attacks. While approaches like gradient masking, defensive distillation have shown some promise, adversarial training seems to be the state-of-the-art approach for defense against adversarial attacks. The success of adversarial training, however, relies on being able to generate large amount of high quality adversarial data. But generating high quality adversarial data in discrete settings like text is %extremely 
hard.
\section{Motivation}
There are two main research directions that have been explored: 
%One set of approaches 
One approach is projecting discrete data onto an embedding space and then applying the popular gradient based adversarial attack algorithms designed for continuous data (like images) on the embeddings ~\cite{miyato2016adversarial, gong2018adversarial}. While these methods are efficient, they suffer from poor success rate.\\
The second research direction is to syntactically perturb the input by finding suitable replacement of individual features like words or characters. However, since the space of possible combinations of substitutions grows exponentially with the length of input data, finding the optimal combination of substitutions %is %intractable.
%a combinatorial 
becomes computationally expensive. 
Some of the recent syntactic perturbation based attacks on NLP classifiers operate by greedy character-level or word-level replacements %which is usually slow, and itâ€™s theoretically not understood when they achieve good performance. 
which is usually fast, but the quality of generated examples may not be very good.

%\section{Syntactic Perturbation}
\section{Our method}
\label{syntactic_continuous}
Consider a sentence $s$ consisting of $n$ words - $[w_{1},\ldots,w_{n}]$. Assume that the $i^{\text{th}}$ word $w_{i} \in \{w_{i}^{(1)},\ldots,w_{i}^{(k_{i})}\}$, i.e. it has $(k_{i}-1)$ synonyms. Further, assume that we are given the adversarial loss function $C(s)$. The goal is to construct a sentence $s'$ consisting of the words of $s$ or its synonyms, such $C(s')$ is maximized. Mathematically, this can be expressed as follows:
\[\text{Find $s' = [\sum_{j=1}^{k_{1}}\alpha_{1}^{(j)}w_{1}^{(j)},\ldots,\sum_{j=1}^{k_{n}}\alpha_{n}^{(j)}w_{n}^{(j)}]$}\]
\[\text{which maximizes $C(s')$ such that}\]
\[\sum_{j=1}^{k_{i}}\alpha_{i}^{(j)} = 1 \text{ }\forall \text{ }i \in [n] \text{ and}\]
\[\alpha_{i}^{(j)} = \{0,1\} \text{ }\forall \text{ }i \in [n], \text{ }j \in [k_{i}]\]
Due to the nature of the constraints, it is clear that the above is a discrete optimization problem which needs to be solved with brute force to find the optimal solution. %Recently, ~\cite{lei2018discrete} posed this in a sub-modular optimization framework and proved a $ 1 - \frac{1}{e}$ approximation factor for attacks that use the greedy algorithm. \\
Recently, ~\cite{lei2018discrete} showed that the above problem for recurrent neural network architectures is a sub-modular optimization problem %due to which greedy methods 
and proved a $(1 - \frac{1}{e})$ approximation factor for attacks that use the greedy algorithm. 

%We propose a further improvement of this by framing it as a continuous optimization problem. While this still gives similar 1-1/e bound to be within the optima but should improve the efficiency multi-fold.
We relax the above problem to an approximately equivalent continuous optimization problem which can be efficiently solved using gradient-based methods and can potentially generate much better adversarial examples. Specifically, we look to solve the following problem:
\[\text{Find 
\small
$s' = [\sum_{j=1}^{k_{1}}\frac{{\big(\beta_{1}^{(j)}\big)}^{2p}}{Z_{1}}w_{1}^{(j)},\ldots,\sum_{j=1}^{k_{n}}\frac{{(\beta_{n}^{(j)})}^{2p}}{Z_{n}}w_{n}^{(j)}]$}
\normalsize
\]
\[\text{where 
\small
$Z_{i} = \sum_{j=1}^{k_{i}}{\big(\beta_{i}^{(j)}\big)}^{2p}$
\normalsize
\text{ and $p \in \mathbb{N}$}
}\]
\[\text{which minimizes } -C(s') + \lambda \sum_{i=1}^{n}\sum_{j=1}^{k_{i}}|\beta_{i}^{(j)}|\]
In the above objective function, the L1-penalty imposed on the $\beta_{i}^{(j)}$'s encourages their sparsity. More importantly, we can use gradient based methods to minimize the above objective function. Of course, the recovered solution is not guaranteed to be one-hot like we want. In other words, relating to the previously stated discrete version of the problem, we would have
\small
$\alpha_{i}^{(j)} = {\big(\beta_{i}^{(j)}\big)}^{2p}/Z_{i}$
\normalsize
which is not guaranteed to be either 0 or 1. To handle this, we greedily do the following:
\[\text{let $j^{*}(i) = \argmax_{j \in [k_{i}]} |{\beta_{i}^{(j)}}|$}\]
\[\text{then $\alpha_{i}^{(j)} = \delta(j - j^{*}(i))$.}\]
The above method can be also used for finding adversarial examples at document-level. In this case, we assume that a document consists of a set of sentences which have some equivalent (in terms of meaning) paraphrases. %The goal is to then identify a combination of the sentence paraphrases which maximizes the adversarial loss function.
Everything remains the same except that the sentence ($s$) is replaced by a document and the set of words constituting the sentence (i.e. $[w_{1},\ldots,w_{n}]$) is replaced by a set of sentences constituting the document. 
\\
\\
\textbf{An example of our method in action}: We trained a simple sentiment classifier using a two-layered bi-directional LSTM. We considered the following sentence ({$s$}) -
``\textbf{The last movie was just brilliant and we liked it}''. This is a sentence with positive sentiment and thus its label is 1.
\\
We considered the following replacements (picked manually for now, will be automated in the future) for every word in the above sentence:
%\\
\begin{itemize}
    \item The - The, This, That, His, Her, Their
    \item last - last, previous
    \item movie - movie, film
    \item was - was, is
    \item just- just, simply, very, quite, pretty
    \item brilliant - brilliant, exceptional, marvelous, sensational, genius, excellent, superb, awesome, splendid, spectacular
    \item and - and, `,'
    \item we - we, I, he, she, they, everyone, everybody
    \item liked - liked, loved, enjoyed
    \item it - it
\end{itemize}
%We used the above set of synonyms to run our method. 
In our method, we used $p = 4$ and $\lambda = 0.002$. %The negative log likelihood of the sentence label was used as $C()$. 
The function $C(.)$ was set equal to the negative log likelihood of the sentence label. 
We used Adam optimizer in our code. %The number of iterations was 30. 
\\
The adversarial example ({$s'$}) generated by our method is (the changed words are in \textcolor{blue}{blue}):
``\textbf{\textcolor{blue}{This} \textcolor{blue}{previous} movie was just \textcolor{blue}{sensational}\textcolor{blue}{,} \textcolor{blue}{they} liked it}". To provide numerical comparison, for $s$, %the negative log likelihood value of $S$ given that its true label is 1 (pos)
the value of $C(s)$ was 0.026 and thus, the probability of $s$ having label 1 (its true label) predicted by our LSTM classifier was $0.974$. However for $s'$, the value of $C(s')$ was $1.8801$ and so the predicted probability of $s'$ having label 1 (true label), %predicted by our LSTM classifier 
was just $0.153$.
\\
\textbf{Evaluation metric for our method}: We will run our method on a set of examples %with an initial accuracy of $A$ (i.e. the percentage of examples correctly classified by the model that we are attacking), and 
for which the mean negative log likelihood and mean probability of belonging to the correct class (predicted by the model that we are attacking) are say, $L$ and $p$, respectively. After running our method, we get a set of adversarial examples for which say, the mean negative log likelihood and mean probability of belonging to the correct class (predicted by the model) are $L'$ and $p'$, respectively. %We want the difference between $L'$ and $L$ as well as
We will quantify the goodness of our method, i.e. the quality of the generated adversarial examples by measuring the difference between $L'$ and $L$ and/or between $p'$ and $p$. 




\begin{comment}
\section{Future Work}
\subsection{Optimal Paraphrase Selection}
While syntactic perturbation methods are useful and easy to understand in terms of theory, it is only able to generate paraphrases with same syntactic order. For example, syntactic perturbation can generate \textit{X called Y} as a paraphrase of \textit{X summoned Y} but can never generate a slight variation as in passive voice like \textit{Y was summoned by X}. That being said if a slight syntactic variation ca fool the classifier, these meaning preserving more sophisticated paraphrases can easily fool the classifier as well. In order to build robust NLP models, we might want to generate a rich variation of adversarial examples that goes beyond simple syntactic perturbations.\\
In order to achieve this we simplify the problem of discreet adversarial text generation into solving two sub-problems.
\begin{itemize}
    \item \textbf{Paraphrase Generation} The objective is to generate paraphrases i.e. given an input sentence generate another sentence that have semantically similar meaning. This can be solved as a monolingual Neural Machine Translation problem and can also be formulated as a bidirectional textual entailment problem.
    \item \textbf{Optimal selection} Once we have a set of paraphrases per sentence we can choose the paraphrase that maximizes the adversarial loss. In cases of documents we can use our continuous optimization [~\ref{syntactic_continuous}] framework to syntactically perturb the document where the document is equivalent to the sentence in the algorithm and the sentences are equivalent to the words in the algorithm. 
\end{itemize}
\subsection{Optimal paraphrase generation and selection as a max-min objective}. Finally, we can formulate the entire problem as a single learning problem where the input is a piece of text and the output is an adversarial text example of the same. We pose this as a joint optimization problem where the objective is:
\\ Given, $s_m = [w_{1},\ldots,w_{m}]$ \\
Find $s_n = [w_{1},\ldots,w_{n}]$\\
Let us assume a distance measure (open problem) $dist(s_i, s_j)$ that represents the syntactic meaning similarity and L be the adversarial loss . Then we solve this joint optimization problem: \\
$\underset{s_n}{\text{argmin }} dist(s_m, s_n)$\\
s.t. $L(s_n) \geq L(s_i),  \forall s_i \in S$ 
\end{comment}

\bibliography{acl2019}
\bibliographystyle{acl_natbib}
\end{document}
